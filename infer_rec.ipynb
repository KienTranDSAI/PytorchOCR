{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "__dir__ = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append(__dir__)\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n",
    "\n",
    "os.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.infer.pytorchocr_utility as utility\n",
    "import copy\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "from pytorchocr.base_ocr_v20 import BaseOCRV20\n",
    "import tools.infer.pytorchocr_utility as utility\n",
    "from pytorchocr.postprocess import build_post_process\n",
    "from pytorchocr.utils.utility import get_image_file_list, check_and_read_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRecognizer(BaseOCRV20):\n",
    "    def __init__(self, args, **kwargs):\n",
    "        self.rec_image_shape = [int(v) for v in args.rec_image_shape.split(\",\")]\n",
    "        self.character_type = args.rec_char_type\n",
    "        self.rec_batch_num = args.rec_batch_num\n",
    "        self.rec_algorithm = args.rec_algorithm\n",
    "        self.max_text_length = args.max_text_length\n",
    "        postprocess_params = {\n",
    "            'name': 'CTCLabelDecode',\n",
    "            \"character_type\": args.rec_char_type,\n",
    "            \"character_dict_path\": args.rec_char_dict_path,\n",
    "            \"use_space_char\": args.use_space_char\n",
    "        }\n",
    "        if self.rec_algorithm == \"SRN\":\n",
    "            postprocess_params = {\n",
    "                'name': 'SRNLabelDecode',\n",
    "                \"character_type\": args.rec_char_type,\n",
    "                \"character_dict_path\": args.rec_char_dict_path,\n",
    "                \"use_space_char\": args.use_space_char\n",
    "            }\n",
    "        elif self.rec_algorithm == \"RARE\":\n",
    "            postprocess_params = {\n",
    "                'name': 'AttnLabelDecode',\n",
    "                \"character_type\": args.rec_char_type,\n",
    "                \"character_dict_path\": args.rec_char_dict_path,\n",
    "                \"use_space_char\": args.use_space_char\n",
    "            }\n",
    "        elif self.rec_algorithm == 'NRTR':\n",
    "            postprocess_params = {\n",
    "                'name': 'NRTRLabelDecode',\n",
    "                \"character_dict_path\": args.rec_char_dict_path,\n",
    "                \"use_space_char\": args.use_space_char\n",
    "            }\n",
    "        elif self.rec_algorithm == \"SAR\":\n",
    "            postprocess_params = {\n",
    "                'name': 'SARLabelDecode',\n",
    "                \"character_dict_path\": args.rec_char_dict_path,\n",
    "                \"use_space_char\": args.use_space_char\n",
    "            }\n",
    "        elif self.rec_algorithm == 'ViTSTR':\n",
    "            postprocess_params = {\n",
    "                'name': 'ViTSTRLabelDecode',\n",
    "                \"character_dict_path\": args.rec_char_dict_path,\n",
    "                \"use_space_char\": args.use_space_char\n",
    "            }\n",
    "        elif self.rec_algorithm == \"CAN\":\n",
    "            self.inverse = args.rec_image_inverse\n",
    "            postprocess_params = {\n",
    "                'name': 'CANLabelDecode',\n",
    "                \"character_dict_path\": args.rec_char_dict_path,\n",
    "                \"use_space_char\": args.use_space_char\n",
    "            }\n",
    "        elif self.rec_algorithm == 'RFL':\n",
    "            postprocess_params = {\n",
    "                'name': 'RFLLabelDecode',\n",
    "                \"character_dict_path\": None,\n",
    "                \"use_space_char\": args.use_space_char\n",
    "            }\n",
    "        elif self.rec_algorithm == \"CRNN\":\n",
    "            postprocess_params = {\n",
    "                'name': 'CTCLabelDecode',\n",
    "                \"character_dict_path\": args.rec_char_dict_path,\n",
    "                \"use_space_char\": args.use_space_char\n",
    "            }\n",
    "        print(args.rec_char_dict_path)\n",
    "        self.postprocess_op = build_post_process(postprocess_params)\n",
    "\n",
    "        use_gpu = args.use_gpu\n",
    "        self.use_gpu = torch.cuda.is_available() and use_gpu\n",
    "\n",
    "        self.limited_max_width = args.limited_max_width\n",
    "        self.limited_min_width = args.limited_min_width\n",
    "\n",
    "        self.weights_path = args.rec_model_path\n",
    "        self.yaml_path = args.rec_yaml_path\n",
    "\n",
    "        char_num = len(getattr(self.postprocess_op, 'character'))\n",
    "        network_config = utility.AnalysisConfig(self.weights_path, self.yaml_path, char_num)\n",
    "        weights = self.read_pytorch_weights(self.weights_path)\n",
    "\n",
    "        self.out_channels = self.get_out_channels(weights)\n",
    "        if self.rec_algorithm == 'NRTR':\n",
    "            self.out_channels = list(weights.values())[-1].numpy().shape[0]\n",
    "        elif self.rec_algorithm == 'SAR':\n",
    "            self.out_channels = list(weights.values())[-3].numpy().shape[0]\n",
    "\n",
    "        kwargs['out_channels'] = self.out_channels\n",
    "        super(TextRecognizer, self).__init__(network_config, **kwargs)\n",
    "\n",
    "        self.load_state_dict(weights)\n",
    "        self.net.eval()\n",
    "        if self.use_gpu:\n",
    "            self.net.cuda()\n",
    "\n",
    "    def resize_norm_img(self, img, max_wh_ratio):\n",
    "        imgC, imgH, imgW = self.rec_image_shape\n",
    "        if self.rec_algorithm == 'NRTR' or self.rec_algorithm == 'ViTSTR':\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            # return padding_im\n",
    "            image_pil = Image.fromarray(np.uint8(img))\n",
    "            if self.rec_algorithm == 'ViTSTR':\n",
    "                img = image_pil.resize([imgW, imgH], Image.BICUBIC)\n",
    "            else:\n",
    "                img = image_pil.resize([imgW, imgH], Image.ANTIALIAS)\n",
    "            img = np.array(img)\n",
    "            norm_img = np.expand_dims(img, -1)\n",
    "            norm_img = norm_img.transpose((2, 0, 1))\n",
    "            if self.rec_algorithm == 'ViTSTR':\n",
    "                norm_img = norm_img.astype(np.float32) / 255.\n",
    "            else:\n",
    "                norm_img = norm_img.astype(np.float32) / 128. - 1.\n",
    "            return norm_img\n",
    "        elif self.rec_algorithm == 'RFL':\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            resized_image = cv2.resize(\n",
    "                img, (imgW, imgH), interpolation=cv2.INTER_CUBIC)\n",
    "            resized_image = resized_image.astype('float32')\n",
    "            resized_image = resized_image / 255\n",
    "            resized_image = resized_image[np.newaxis, :]\n",
    "            resized_image -= 0.5\n",
    "            resized_image /= 0.5\n",
    "            return resized_image\n",
    "\n",
    "        assert imgC == img.shape[2]\n",
    "        max_wh_ratio = max(max_wh_ratio, imgW / imgH)\n",
    "        imgW = int((imgH * max_wh_ratio))\n",
    "        imgW = max(min(imgW, self.limited_max_width), self.limited_min_width)\n",
    "        h, w = img.shape[:2]\n",
    "        ratio = w / float(h)\n",
    "        ratio_imgH = math.ceil(imgH * ratio)\n",
    "        ratio_imgH = max(ratio_imgH, self.limited_min_width)\n",
    "        if ratio_imgH > imgW:\n",
    "            resized_w = imgW\n",
    "        else:\n",
    "            resized_w = int(ratio_imgH)\n",
    "        resized_image = cv2.resize(img, (resized_w, imgH))\n",
    "        resized_image = resized_image.astype('float32')\n",
    "        resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
    "        resized_image -= 0.5\n",
    "        resized_image /= 0.5\n",
    "        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
    "        padding_im[:, :, 0:resized_w] = resized_image\n",
    "        return padding_im\n",
    "\n",
    "    def resize_norm_img_svtr(self, img, image_shape):\n",
    "\n",
    "        imgC, imgH, imgW = image_shape\n",
    "        resized_image = cv2.resize(\n",
    "            img, (imgW, imgH), interpolation=cv2.INTER_LINEAR)\n",
    "        resized_image = resized_image.astype('float32')\n",
    "        resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
    "        resized_image -= 0.5\n",
    "        resized_image /= 0.5\n",
    "        return resized_image\n",
    "\n",
    "\n",
    "    def resize_norm_img_srn(self, img, image_shape):\n",
    "        imgC, imgH, imgW = image_shape\n",
    "\n",
    "        img_black = np.zeros((imgH, imgW))\n",
    "        im_hei = img.shape[0]\n",
    "        im_wid = img.shape[1]\n",
    "\n",
    "        if im_wid <= im_hei * 1:\n",
    "            img_new = cv2.resize(img, (imgH * 1, imgH))\n",
    "        elif im_wid <= im_hei * 2:\n",
    "            img_new = cv2.resize(img, (imgH * 2, imgH))\n",
    "        elif im_wid <= im_hei * 3:\n",
    "            img_new = cv2.resize(img, (imgH * 3, imgH))\n",
    "        else:\n",
    "            img_new = cv2.resize(img, (imgW, imgH))\n",
    "\n",
    "        img_np = np.asarray(img_new)\n",
    "        img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n",
    "        img_black[:, 0:img_np.shape[1]] = img_np\n",
    "        img_black = img_black[:, :, np.newaxis]\n",
    "\n",
    "        row, col, c = img_black.shape\n",
    "        c = 1\n",
    "\n",
    "        return np.reshape(img_black, (c, row, col)).astype(np.float32)\n",
    "\n",
    "    def srn_other_inputs(self, image_shape, num_heads, max_text_length):\n",
    "\n",
    "        imgC, imgH, imgW = image_shape\n",
    "        feature_dim = int((imgH / 8) * (imgW / 8))\n",
    "\n",
    "        encoder_word_pos = np.array(range(0, feature_dim)).reshape(\n",
    "            (feature_dim, 1)).astype('int64')\n",
    "        gsrm_word_pos = np.array(range(0, max_text_length)).reshape(\n",
    "            (max_text_length, 1)).astype('int64')\n",
    "\n",
    "        gsrm_attn_bias_data = np.ones((1, max_text_length, max_text_length))\n",
    "        gsrm_slf_attn_bias1 = np.triu(gsrm_attn_bias_data, 1).reshape(\n",
    "            [-1, 1, max_text_length, max_text_length])\n",
    "        gsrm_slf_attn_bias1 = np.tile(\n",
    "            gsrm_slf_attn_bias1,\n",
    "            [1, num_heads, 1, 1]).astype('float32') * [-1e9]\n",
    "\n",
    "        gsrm_slf_attn_bias2 = np.tril(gsrm_attn_bias_data, -1).reshape(\n",
    "            [-1, 1, max_text_length, max_text_length])\n",
    "        gsrm_slf_attn_bias2 = np.tile(\n",
    "            gsrm_slf_attn_bias2,\n",
    "            [1, num_heads, 1, 1]).astype('float32') * [-1e9]\n",
    "\n",
    "        encoder_word_pos = encoder_word_pos[np.newaxis, :]\n",
    "        gsrm_word_pos = gsrm_word_pos[np.newaxis, :]\n",
    "\n",
    "        return [\n",
    "            encoder_word_pos, gsrm_word_pos, gsrm_slf_attn_bias1,\n",
    "            gsrm_slf_attn_bias2\n",
    "        ]\n",
    "\n",
    "    def process_image_srn(self, img, image_shape, num_heads, max_text_length):\n",
    "        norm_img = self.resize_norm_img_srn(img, image_shape)\n",
    "        norm_img = norm_img[np.newaxis, :]\n",
    "\n",
    "        [encoder_word_pos, gsrm_word_pos, gsrm_slf_attn_bias1, gsrm_slf_attn_bias2] = \\\n",
    "            self.srn_other_inputs(image_shape, num_heads, max_text_length)\n",
    "\n",
    "        gsrm_slf_attn_bias1 = gsrm_slf_attn_bias1.astype(np.float32)\n",
    "        gsrm_slf_attn_bias2 = gsrm_slf_attn_bias2.astype(np.float32)\n",
    "        encoder_word_pos = encoder_word_pos.astype(np.int64)\n",
    "        gsrm_word_pos = gsrm_word_pos.astype(np.int64)\n",
    "\n",
    "        return (norm_img, encoder_word_pos, gsrm_word_pos, gsrm_slf_attn_bias1,\n",
    "                gsrm_slf_attn_bias2)\n",
    "\n",
    "    def resize_norm_img_sar(self, img, image_shape,\n",
    "                            width_downsample_ratio=0.25):\n",
    "        imgC, imgH, imgW_min, imgW_max = image_shape\n",
    "        h = img.shape[0]\n",
    "        w = img.shape[1]\n",
    "        valid_ratio = 1.0\n",
    "        # make sure new_width is an integral multiple of width_divisor.\n",
    "        width_divisor = int(1 / width_downsample_ratio)\n",
    "        # resize\n",
    "        ratio = w / float(h)\n",
    "        resize_w = math.ceil(imgH * ratio)\n",
    "        if resize_w % width_divisor != 0:\n",
    "            resize_w = round(resize_w / width_divisor) * width_divisor\n",
    "        if imgW_min is not None:\n",
    "            resize_w = max(imgW_min, resize_w)\n",
    "        if imgW_max is not None:\n",
    "            valid_ratio = min(1.0, 1.0 * resize_w / imgW_max)\n",
    "            resize_w = min(imgW_max, resize_w)\n",
    "        resized_image = cv2.resize(img, (resize_w, imgH))\n",
    "        resized_image = resized_image.astype('float32')\n",
    "        # norm\n",
    "        if image_shape[0] == 1:\n",
    "            resized_image = resized_image / 255\n",
    "            resized_image = resized_image[np.newaxis, :]\n",
    "        else:\n",
    "            resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
    "        resized_image -= 0.5\n",
    "        resized_image /= 0.5\n",
    "        resize_shape = resized_image.shape\n",
    "        padding_im = -1.0 * np.ones((imgC, imgH, imgW_max), dtype=np.float32)\n",
    "        padding_im[:, :, 0:resize_w] = resized_image\n",
    "        pad_shape = padding_im.shape\n",
    "\n",
    "        return padding_im, resize_shape, pad_shape, valid_ratio\n",
    "\n",
    "\n",
    "    def norm_img_can(self, img, image_shape):\n",
    "\n",
    "        img = cv2.cvtColor(\n",
    "            img, cv2.COLOR_BGR2GRAY)  # CAN only predict gray scale image\n",
    "\n",
    "        if self.inverse:\n",
    "            img = 255 - img\n",
    "\n",
    "        if self.rec_image_shape[0] == 1:\n",
    "            h, w = img.shape\n",
    "            _, imgH, imgW = self.rec_image_shape\n",
    "            if h < imgH or w < imgW:\n",
    "                padding_h = max(imgH - h, 0)\n",
    "                padding_w = max(imgW - w, 0)\n",
    "                img_padded = np.pad(img, ((0, padding_h), (0, padding_w)),\n",
    "                                    'constant',\n",
    "                                    constant_values=(255))\n",
    "                img = img_padded\n",
    "\n",
    "        img = np.expand_dims(img, 0) / 255.0  # h,w,c -> c,h,w\n",
    "        img = img.astype('float32')\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __call__(self, img_list):\n",
    "        img_num = len(img_list)\n",
    "        # Calculate the aspect ratio of all text bars\n",
    "        width_list = []\n",
    "        for img in img_list:\n",
    "            width_list.append(img.shape[1] / float(img.shape[0]))\n",
    "        # Sorting can speed up the recognition process\n",
    "        indices = np.argsort(np.array(width_list))\n",
    "\n",
    "        # rec_res = []\n",
    "        rec_res = [['', 0.0]] * img_num\n",
    "        batch_num = self.rec_batch_num\n",
    "        elapse = 0\n",
    "        for beg_img_no in range(0, img_num, batch_num):\n",
    "            end_img_no = min(img_num, beg_img_no + batch_num)\n",
    "            norm_img_batch = []\n",
    "            max_wh_ratio = 0\n",
    "            for ino in range(beg_img_no, end_img_no):\n",
    "                # h, w = img_list[ino].shape[0:2]\n",
    "                h, w = img_list[indices[ino]].shape[0:2]\n",
    "                wh_ratio = w * 1.0 / h\n",
    "                max_wh_ratio = max(max_wh_ratio, wh_ratio)\n",
    "            for ino in range(beg_img_no, end_img_no):\n",
    "                if self.rec_algorithm == \"SAR\":\n",
    "                    norm_img, _, _, valid_ratio = self.resize_norm_img_sar(\n",
    "                        img_list[indices[ino]], self.rec_image_shape)\n",
    "                    norm_img = norm_img[np.newaxis, :]\n",
    "                    valid_ratio = np.expand_dims(valid_ratio, axis=0)\n",
    "                    valid_ratios = []\n",
    "                    valid_ratios.append(valid_ratio)\n",
    "                    norm_img_batch.append(norm_img)\n",
    "\n",
    "                elif self.rec_algorithm == \"SVTR\":\n",
    "                    norm_img = self.resize_norm_img_svtr(img_list[indices[ino]],\n",
    "                                                         self.rec_image_shape)\n",
    "                    norm_img = norm_img[np.newaxis, :]\n",
    "                    norm_img_batch.append(norm_img)\n",
    "                elif self.rec_algorithm == \"SRN\":\n",
    "                    norm_img = self.process_image_srn(img_list[indices[ino]],\n",
    "                                                      self.rec_image_shape, 8,\n",
    "                                                      self.max_text_length)\n",
    "                    encoder_word_pos_list = []\n",
    "                    gsrm_word_pos_list = []\n",
    "                    gsrm_slf_attn_bias1_list = []\n",
    "                    gsrm_slf_attn_bias2_list = []\n",
    "                    encoder_word_pos_list.append(norm_img[1])\n",
    "                    gsrm_word_pos_list.append(norm_img[2])\n",
    "                    gsrm_slf_attn_bias1_list.append(norm_img[3])\n",
    "                    gsrm_slf_attn_bias2_list.append(norm_img[4])\n",
    "                    norm_img_batch.append(norm_img[0])\n",
    "                elif self.rec_algorithm == \"CAN\":\n",
    "                    norm_img = self.norm_img_can(img_list[indices[ino]],\n",
    "                                                 max_wh_ratio)\n",
    "                    norm_img = norm_img[np.newaxis, :]\n",
    "                    norm_img_batch.append(norm_img)\n",
    "                    norm_image_mask = np.ones(norm_img.shape, dtype='float32')\n",
    "                    word_label = np.ones([1, 36], dtype='int64')\n",
    "                    norm_img_mask_batch = []\n",
    "                    word_label_list = []\n",
    "                    norm_img_mask_batch.append(norm_image_mask)\n",
    "                    word_label_list.append(word_label)\n",
    "                else:\n",
    "                    norm_img = self.resize_norm_img(img_list[indices[ino]],\n",
    "                                                    max_wh_ratio)\n",
    "                    norm_img = norm_img[np.newaxis, :]\n",
    "                    norm_img_batch.append(norm_img)\n",
    "            norm_img_batch = np.concatenate(norm_img_batch)\n",
    "            norm_img_batch = norm_img_batch.copy()\n",
    "\n",
    "            if self.rec_algorithm == \"SRN\":\n",
    "                starttime = time.time()\n",
    "                encoder_word_pos_list = np.concatenate(encoder_word_pos_list)\n",
    "                gsrm_word_pos_list = np.concatenate(gsrm_word_pos_list)\n",
    "                gsrm_slf_attn_bias1_list = np.concatenate(\n",
    "                    gsrm_slf_attn_bias1_list)\n",
    "                gsrm_slf_attn_bias2_list = np.concatenate(\n",
    "                    gsrm_slf_attn_bias2_list)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    inp = torch.from_numpy(norm_img_batch)\n",
    "                    encoder_word_pos_inp = torch.from_numpy(encoder_word_pos_list)\n",
    "                    gsrm_word_pos_inp = torch.from_numpy(gsrm_word_pos_list)\n",
    "                    gsrm_slf_attn_bias1_inp = torch.from_numpy(gsrm_slf_attn_bias1_list)\n",
    "                    gsrm_slf_attn_bias2_inp = torch.from_numpy(gsrm_slf_attn_bias2_list)\n",
    "\n",
    "                    if self.use_gpu:\n",
    "                        inp = inp.cuda()\n",
    "                        encoder_word_pos_inp = encoder_word_pos_inp.cuda()\n",
    "                        gsrm_word_pos_inp = gsrm_word_pos_inp.cuda()\n",
    "                        gsrm_slf_attn_bias1_inp = gsrm_slf_attn_bias1_inp.cuda()\n",
    "                        gsrm_slf_attn_bias2_inp = gsrm_slf_attn_bias2_inp.cuda()\n",
    "\n",
    "                    backbone_out = self.net.backbone(inp) # backbone_feat\n",
    "                    prob_out = self.net.head(backbone_out, [encoder_word_pos_inp, gsrm_word_pos_inp, gsrm_slf_attn_bias1_inp, gsrm_slf_attn_bias2_inp])\n",
    "                # preds = {\"predict\": prob_out[2]}\n",
    "                preds = {\"predict\": prob_out[\"predict\"]}\n",
    "\n",
    "            elif self.rec_algorithm == \"SAR\":\n",
    "                starttime = time.time()\n",
    "                # valid_ratios = np.concatenate(valid_ratios)\n",
    "                # inputs = [\n",
    "                #     norm_img_batch,\n",
    "                #     valid_ratios,\n",
    "                # ]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    inp = torch.from_numpy(norm_img_batch)\n",
    "                    if self.use_gpu:\n",
    "                        inp = inp.cuda()\n",
    "                    preds = self.net(inp)\n",
    "\n",
    "            elif self.rec_algorithm == \"CAN\":\n",
    "                starttime = time.time()\n",
    "                norm_img_mask_batch = np.concatenate(norm_img_mask_batch)\n",
    "                word_label_list = np.concatenate(word_label_list)\n",
    "                inputs = [norm_img_batch, norm_img_mask_batch, word_label_list]\n",
    "\n",
    "                inp = [torch.from_numpy(e_i) for e_i in inputs]\n",
    "                if self.use_gpu:\n",
    "                    inp = [e_i.cuda() for e_i in inp]\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.net(inp)\n",
    "                    outputs = [v.cpu().numpy() for k, v in enumerate(outputs)]\n",
    "\n",
    "                preds = outputs\n",
    "\n",
    "            else:\n",
    "                starttime = time.time()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    inp = torch.from_numpy(norm_img_batch)\n",
    "                    if self.use_gpu:\n",
    "                        inp = inp.cuda()\n",
    "                    prob_out = self.net(inp)\n",
    "\n",
    "                if isinstance(prob_out, list):\n",
    "                    preds = [v.cpu().numpy() for v in prob_out]\n",
    "                else:\n",
    "                    preds = prob_out.cpu().numpy()\n",
    "\n",
    "            rec_result = self.postprocess_op(preds)\n",
    "            for rno in range(len(rec_result)):\n",
    "                rec_res[indices[beg_img_no + rno]] = rec_result[rno]\n",
    "            elapse += time.time() - starttime\n",
    "        return rec_res, elapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/kientdt/shared_drive_cv/ocr/kientdt/PaddleOCR/ppocr/utils/dict/my_vi_dict.txt\n",
      "weights is loaded.\n"
     ]
    }
   ],
   "source": [
    "args = utility.init_args().parse_args(args=[])\n",
    "args.rec_model_path = \"/raid/kientdt/shared_drive_cv/ocr/kientdt/PaddleOCR2Pytorch/vi_with_number_PP-OCRv3_ptocr_v3_rec_infer.pth\"\n",
    "# args.image_dir = \"/raid/kientdt/shared_drive_cv/ocr/kientdt/PaddleOCR/ocr_text_recog_hieunq_v2/Arial_Bold/train/img_10.jpg\"\n",
    "args.rec_char_dict_path = \"/raid/kientdt/shared_drive_cv/ocr/kientdt/PaddleOCR/ppocr/utils/dict/my_vi_dict.txt\"\n",
    "text_recognizer = TextRecognizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=77x40>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = \"/raid/kientdt/shared_drive_cv/ocr/kientdt/PaddleOCR/ocr_text_recog_hieunq_v2/Arial_Bold/train/img_169.jpg\"\n",
    "Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('xuáº¥t,', 0.98775053)], 0.0070056915283203125)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(img_path)\n",
    "img = [img]\n",
    "results = text_recognizer(img)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
